{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IP Agreement Clause Generator\n",
    "\n",
    "\n",
    "This generator involves developing a system to generate new clauses for intellectual property (IP) agreements used by lawyers. Generator ensures these clauses incorporate prior art and best practices based on existing data from law firms. The generated clauses  lead to favorable economic and legal outcomes for the assignee.\n",
    "\n",
    "Lawyers can input requests within the context of an original document. The system retrieves similar clauses from a dataset, ranks them from best to worst based on their outcomes, and generates a clause that can be seamlessly integrated into the original document. The best clause is defined as one with the most favorable economic and legal outcomes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Langchain dependencies\n",
    "from langchain.document_loaders.pdf import PyPDFDirectoryLoader  # Importing PDF loader from Langchain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter  # Importing text splitter from Langchain\n",
    "from langchain.embeddings import OpenAIEmbeddings  # Importing OpenAI embeddings from Langchain\n",
    "from langchain.schema import Document  # Importing Document schema from Langchain\n",
    "from langchain.vectorstores.chroma import Chroma  # Importing Chroma vector store from Langchain\n",
    "from dotenv import load_dotenv # Importing dotenv to get API key from .env file\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import OpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import Document\n",
    "from langchain.vectorstores.chroma import Chroma\n",
    "import fitz\n",
    "import os  # Importing os module for operating system functionalities\n",
    "import shutil  # Importing shutil module for high-level file operations\n",
    "import warnings\n",
    "\n",
    "# Suppress specific warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Split into chunks of text\n",
    "\n",
    "- Function to split text content of documents:\n",
    "- Split the text content of the given list of `Document` objects into smaller chunks:\n",
    " \n",
    "  - Parameters:\n",
    "    - `chunk_size`: Size of each chunk in characters.\n",
    "    - `chunk_overlap`: Overlap between consecutive chunks.\n",
    "    - `length_function`: Function to compute the length of the text.\n",
    "    - `add_start_index`: Flag to add start index to each chunk.\n",
    "  \n",
    "  - Returns:\n",
    "    - Returns a list of `Document` objects representing the split text chunks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(document):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=400,  # Size of each chunk in characters\n",
    "        chunk_overlap=100,  # Overlap between consecutive chunks\n",
    "        length_function=len,  # Function to compute the length of the text\n",
    "        add_start_index=True,  # Flag to add start index to each chunk\n",
    "    )\n",
    "    # Split document into smaller chunks\n",
    "    chunks = text_splitter.split_documents([document])\n",
    "    return [Document(page_content=chunk.page_content) for chunk in chunks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Model Initialization & Prompt Creation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gptPrompt = \"\"\"\n",
    "You are basically lawyer that is basically extracting the legal clauses given the text. Now I will give you a Text and base on the text, you will ruturn a text that contains only the legal clauses and nothing else.\"\n",
    "Text: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "lawyer_template = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=gptPrompt\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Define the GPT Prompt:**\n",
    "  - `gptPrompt` is a string that serves as a template for the task of extracting legal clauses from a given text.\n",
    "  - The template instructs the model to:\n",
    "    - Act as a lawyer.\n",
    "    - Extract and return only the legal clauses from the provided text.\n",
    "  - The placeholder `{question}` will be replaced with the actual text input when used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Load Environment Variables:**\n",
    "  - `load_dotenv()` is a function from the `dotenv` library that loads environment variables from a `.env` file into the environment.\n",
    "  - This function is used to ensure that environment variables (such as API keys, database URLs, or configuration settings) defined in a `.env` file are available to the application.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Initialize OpenAI Model:**\n",
    "  - `llm = OpenAI()` initializes an instance of the OpenAI language model.\n",
    "  - This object represents the language model provided by OpenAI and is used to interact with the model for generating text, answering questions, or performing other language-related tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain= lawyer_template | llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Combine Prompt Template with Model:**\n",
    "  - `llm_chain = lawyer_template | llm` creates a processing chain by combining the `lawyer_template` with the OpenAI model instance (`llm`).\n",
    "  - This uses the `|` operator, which typically represents chaining or combining components in this context.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    final_text = \"\"\n",
    "    for page in doc:\n",
    "        text = page.get_text()\n",
    "        \n",
    "        final_text+=llm_chain.invoke(text)\n",
    "    doc.close()\n",
    "    return final_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - `def extract_text_from_pdf(pdf_path):` defines a function to extract and process text from a PDF file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_all_paths(folder_path):\n",
    "    paths = []\n",
    "    for root, directories, files in os.walk(folder_path):\n",
    "        for name in files:\n",
    "            paths.append(os.path.join(root, name))\n",
    "        for name in directories:\n",
    "            paths.append(os.path.join(root, name))\n",
    "    return paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - `def list_all_paths(folder_path):` defines a function to list all file and directory paths within a specified folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths=list_all_paths(r'D:\\RAG\\Law LLM (RAG)\\1. RAG\\data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Save to a RDB using Chroma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHROMA_PATH = r\"D:\\RAG\\Law LLM (RAG)\\1. RAG\\chroma2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = Chroma(persist_directory=CHROMA_PATH, embedding_function=OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Database Initialization:**\n",
    "  - `CHROMA_PATH = r\"D:\\RAG\\Law LLM (RAG)\\1. RAG\\chroma2\"` sets the path for the Chroma database.\n",
    "  - `db = Chroma(persist_directory=CHROMA_PATH, embedding_function=OpenAIEmbeddings())` initializes a new Chroma database at the specified path using OpenAI embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def save_to_chroma(chunks: list[Document], db):\n",
    "    # Add new chunks to the database\n",
    "    db.add_documents(chunks)\n",
    "    db.persist()\n",
    "    print(f\"Appended {len(chunks)} chunks to {CHROMA_PATH}.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Save to Chroma Function:**\n",
    "  - `def save_to_chroma(chunks: list[Document], db):` defines a function to save document chunks to the Chroma database.\n",
    "  - `db.add_documents(chunks)` adds the new chunks to the database.\n",
    "  - `db.persist()` saves the changes to the database.\n",
    "  - `print(f\"Appended {len(chunks)} chunks to {CHROMA_PATH}.\")` prints the number of chunks added to the database.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=1\n",
    "for path in paths:\n",
    "    text=extract_text_from_pdf(path)\n",
    "    document = Document(page_content=text)\n",
    "    chunks = split_text(document)\n",
    "    save_to_chroma(chunks,db)\n",
    "    print(\"document \"+str(i)+\" processed\")\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Processing and Saving Documents:**\n",
    "  - `i = 1` initializes a counter to keep track of processed documents.\n",
    "  - `for path in paths:` iterates over all paths in the previously generated list.\n",
    "  - `text = extract_text_from_pdf(path)` extracts text from the current PDF path.\n",
    "  - `document = Document(page_content=text)` creates a `Document` object with the extracted text.\n",
    "  - `chunks = split_text(document)` splits the document text into smaller chunks.\n",
    "  - `save_to_chroma(chunks, db)` saves the chunks to the Chroma database.\n",
    "  - `print(\"document \" + str(i) + \" processed\")` prints a message indicating the current document has been processed.\n",
    "  - `i += 1` increments the counter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Query vector database for relevant data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Answer the question based only on the following context:\n",
    "\n",
    "{context}\n",
    "You are required to rank the retrieved clauses from the most favorable economic conditions to the worst conditions. The evaluation should consider factors such as economic stability, growth prospects, inflation rates, employment levels, and other relevant economic indicators. Additionally, the legal outcomes should encompass potential liabilities, risks, and the protection afforded to the assignee under different economic scenarios, including the legal outcomes for the assignee. Do not generate any new clauses. Just rewrite the existing clauses entire text\n",
    "Answer the question based on the following query: {question}\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Prompt Template Definition:**\n",
    "  - `PROMPT_TEMPLATE` is a string that defines a template for answering a question based on provided context.\n",
    "  - The template instructs the model to:\n",
    "    - Rank the retrieved clauses from the most favorable economic conditions to the worst conditions.\n",
    "    - Consider factors such as economic stability, growth prospects, inflation rates, employment levels, and other relevant economic indicators.\n",
    "    - Evaluate potential liabilities, risks, and the protection afforded to the assignee under different economic scenarios, including the legal outcomes for the assignee.\n",
    "    - Only rewrite the existing clauses and not generate any new clauses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User-Input & Generation of Effective Clauses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      " 1. Section 3.01(a) - Honeywell grants SpinCo and its group members a non-exclusive, royalty-free, perpetual, sublicenseable worldwide license to use the Honeywell Shared IP, excluding Trademarks, for the purposes of the SpinCo Business prior to the Distribution Date.\n",
      "\n",
      "2. Section 4.01(a) - SpinCo grants Honeywell and its subsidiaries a non-exclusive, royalty-free, perpetual, sublicenseable worldwide license to use the SpinCo Shared IP for the same purposes as used prior to the Distribution Date.\n",
      "\n",
      "3. Section 3.01(b) - The terms of the Trademark License Agreement shall control in case of any conflicts between that agreement and the terms of this Agreement regarding the use of certain Honeywell Trademarks by the SpinCo Group.\n",
      "\n",
      "4. Section 4.02(b) - SpinCo must promptly notify Honeywell of any infringement issues.\n",
      "\n",
      "5. Section 4.01(b) - Honeywell has the right to challenge the validity or enforceability of a transfer of more than fifty percent of voting power, shares, or equity of a member of the SpinCo Group to a third party.\n"
     ]
    }
   ],
   "source": [
    "query_text=\"\"\"(b) Trademarks. The Parties acknowledge and agree that certain rights and obligations with respect to the use by the SpinCo Group of\n",
    "certain Honeywell Trademarks shall be set forth in the Trademark License Agreement. To the extent there is a conflict between the terms of this\n",
    "Agreement and the Trademark License Agreement, the terms of the Trademark License Agreement shall control.\"\"\"\n",
    "\n",
    "# Search the DB.\n",
    "results = db.similarity_search_with_relevance_scores(query_text, k=10)\n",
    "if len(results) == 0 or results[0][1] < 0.7:\n",
    "    print(f\"Unable to find matching results.\")\n",
    "    \n",
    "    \n",
    "context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc, _score in results])\n",
    "prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "prompt = prompt_template.format(context=context_text, question=query_text)\n",
    "\n",
    "\n",
    "model = ChatOpenAI()\n",
    "response_text = model.predict(prompt)\n",
    "\n",
    "sources = [doc.metadata.get(\"source\", None) for doc, _score in results]\n",
    "formatted_response = f\"Response:\\n {response_text}\"\n",
    "print(formatted_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Query Text Definition:**\n",
    "  - `query_text` is a string that contains the text to be searched in the database.\n",
    "\n",
    "- **Perform Similarity Search:**\n",
    "  - `results = db.similarity_search_with_relevance_scores(query_text, k=10)`\n",
    "    - Searches the database for documents most similar to the `query_text`.\n",
    "    - Retrieves the top 10 results along with their relevance scores.\n",
    "\n",
    "- **Check for Matching Results:**\n",
    "  - `if len(results) == 0 or results[0][1] < 0.7:`\n",
    "    - Checks if no results were found or if the top result's relevance score is below 0.7.\n",
    "    - If either condition is true, prints a message indicating that no matching results were found.\n",
    "    - `print(f\"Unable to find matching results.\")`\n",
    "\n",
    "- **Prepare Context Text:**\n",
    "  - `context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc, _score in results])`\n",
    "    - Joins the page content of the retrieved documents into a single string, with each document's content separated by `\\n\\n---\\n\\n`.\n",
    "\n",
    "- **Create Prompt Template:**\n",
    "  - `prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)`\n",
    "    - Creates a prompt template using the predefined `PROMPT_TEMPLATE`.\n",
    "\n",
    "- **Format the Prompt:**\n",
    "  - `prompt = prompt_template.format(context=context_text, question=query_text)`\n",
    "    - Formats the prompt template with the `context_text` and `query_text`.\n",
    "\n",
    "- **Model Prediction:**\n",
    "  - `model = ChatOpenAI()`\n",
    "    - Initializes the OpenAI language model.\n",
    "  - `response_text = model.predict(prompt)`\n",
    "    - Generates a response from the model based on the formatted prompt.\n",
    "\n",
    "- **Retrieve and Format Sources:**\n",
    "  - `sources = [doc.metadata.get(\"source\", None) for doc, _score in results]`\n",
    "    - Extracts the source metadata from the retrieved documents.\n",
    "  - `formatted_response = f\"Response:\\n {response_text}\"`\n",
    "    - Combines the response text with a formatted message.\n",
    "\n",
    "- **Print Formatted Response:**\n",
    "  - `print(formatted_response)`\n",
    "    - Prints the formatted response to the console.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jobgen_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
